{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30616,"status":"ok","timestamp":1713278816204,"user":{"displayName":"Garrett Devaney","userId":"10600170082792839935"},"user_tz":240},"id":"0IzSVzoi4qxM","outputId":"1e7a1582-c6c3-4219-f168-8851c234747c"},"outputs":[],"source":["# %%bash\n","# # Colab-specific setup\n","# !(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit\n","# pip install yacs 2>&1 >> install.log\n","# git init 2>&1 >> install.log\n","# git remote add origin https://github.com/jeongrok/semantic-segmentation-pytorch.git 2>> install.log\n","# git pull origin master 2>&1 >> install.log\n","# DOWNLOAD_ONLY=1 ./demo_test.sh 2>> install.log"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: opencv-python in /Users/yoon/miniforge3/envs/wall_ai/lib/python3.11/site-packages (4.9.0.80)\n","Requirement already satisfied: numpy>=1.21.2 in /Users/yoon/miniforge3/envs/wall_ai/lib/python3.11/site-packages (from opencv-python) (1.26.4)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["# %pip install --default-timeout=100 future\n","# %pip install opencv-python"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/yoon/miniforge3/envs/wall_ai/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/yoon/miniforge3/envs/wall_ai/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n","  Referenced from: <CFED5F8E-EC3F-36FD-AAA3-2C6C7F8D3DD9> /Users/yoon/miniforge3/envs/wall_ai/lib/python3.11/site-packages/torchvision/image.so\n","  Expected in:     <E459C462-F863-3A5A-AC9F-FD77B14BE845> /Users/yoon/miniforge3/envs/wall_ai/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n","  warn(\n"]}],"source":["import os, csv, torch, numpy as np, scipy.io, PIL.Image, torchvision.transforms, glob, matplotlib.pyplot as plt, cv2\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":6717,"status":"ok","timestamp":1713278822916,"user":{"displayName":"Garrett Devaney","userId":"10600170082792839935"},"user_tz":240},"id":"1MRlIc3i4z1I"},"outputs":[],"source":["# System libs\n","# import os, csv, torch, numpy as np, scipy.io, PIL.Image, torchvision.transforms, glob, matplotlib.pyplot as plt, cv2\n","# Our libs\n","from mit_semseg.models.models import ModelBuilder, SegmentationModule\n","from mit_semseg.utils import colorEncode\n","\n","colors = scipy.io.loadmat('data/color150.mat')['colors']\n","names = {}\n","with open('data/object150_info.csv') as f:\n","    reader = csv.reader(f)\n","    next(reader)\n","    for row in reader:\n","        names[int(row[0])] = row[5].split(\";\")[0]"]},{"cell_type":"code","execution_count":49,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1230,"status":"ok","timestamp":1713278824143,"user":{"displayName":"Garrett Devaney","userId":"10600170082792839935"},"user_tz":240},"id":"BkY_uG2W49Ht","outputId":"ce45beb3-6392-4530-c751-e1ee7459190c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading weights for net_encoder\n","Loading weights for net_decoder\n"]},{"ename":"TypeError","evalue":"SegmentationModule.__init__() takes 3 positional arguments but 4 were given","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[49], line 17\u001b[0m\n\u001b[1;32m      8\u001b[0m net_decoder \u001b[38;5;241m=\u001b[39m ModelBuilder\u001b[38;5;241m.\u001b[39mbuild_decoder(\n\u001b[1;32m      9\u001b[0m     arch\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mppm_deepsup\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     10\u001b[0m     fc_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/yoon/Documents/gatech/spring24/WallAI/semantic_segmentation_pytorch/ckpt/ade20k-resnet50dilated-ppm_deepsup/decoder_epoch_20.pth\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     14\u001b[0m     use_softmax\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m crit \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mNLLLoss(ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m segmentation_module \u001b[38;5;241m=\u001b[39m SegmentationModule(net_encoder, net_decoder, crit)\u001b[38;5;66;03m#.to(DEVICE)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m segmentation_module\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     19\u001b[0m DEVICE \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mTypeError\u001b[0m: SegmentationModule.__init__() takes 3 positional arguments but 4 were given"]}],"source":["# Network Builders\n","# segmentation\n","net_encoder = ModelBuilder.build_encoder(\n","    arch='resnet50dilated',\n","    fc_dim=2048,\n","    # weights=weights_encoder\n","    weights='/Users/yoon/Documents/gatech/spring24/WallAI/semantic_segmentation_pytorch/ckpt/ade20k-resnet50dilated-ppm_deepsup/encoder_epoch_20.pth')\n","net_decoder = ModelBuilder.build_decoder(\n","    arch='ppm_deepsup',\n","    fc_dim=2048,\n","    num_class=150,\n","    # weights=weights_decoder,\n","    weights='/Users/yoon/Documents/gatech/spring24/WallAI/semantic_segmentation_pytorch/ckpt/ade20k-resnet50dilated-ppm_deepsup/decoder_epoch_20.pth',\n","    use_softmax=True)\n","\n","crit = torch.nn.NLLLoss(ignore_index=-1)\n","segmentation_module = SegmentationModule(net_encoder, net_decoder, crit)#.to(DEVICE)\n","segmentation_module.eval()\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","segmentation_module.to(DEVICE)"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading weights for net_encoder\n","Loading weights for net_decoder\n"]},{"ename":"TypeError","evalue":"SegmentationModule.__init__() takes 3 positional arguments but 4 were given","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[51], line 19\u001b[0m\n\u001b[1;32m     10\u001b[0m pspnet_decoder \u001b[38;5;241m=\u001b[39m ModelBuilder\u001b[38;5;241m.\u001b[39mbuild_decoder(\n\u001b[1;32m     11\u001b[0m     arch\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mppm_deepsup\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     12\u001b[0m     fc_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/yoon/Documents/gatech/spring24/WallAI/semantic_segmentation_pytorch/ckpt/ade20k-resnet50dilated-ppm_deepsup/decoder_epoch_20.pth\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     16\u001b[0m     use_softmax\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m crit \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mNLLLoss(ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m segmentation_module \u001b[38;5;241m=\u001b[39m SegmentationModule(net_encoder, net_decoder, crit)\u001b[38;5;66;03m#.to(DEVICE)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m segmentation_module\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     21\u001b[0m DEVICE \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mTypeError\u001b[0m: SegmentationModule.__init__() takes 3 positional arguments but 4 were given"]}],"source":["#pspnet\n","weights_encoder = '/Users/yoon/Documents/gatech/spring24/WallSegmentation/model_weights/Transfer learning - entire decoder/transfer_encoder.pth'\n","weights_decoder = '/Users/yoon/Documents/gatech/spring24/WallSegmentation/model_weights/Transfer learning - entire decoder/transfer_decoder.pth'\n","\n","pspnet_encoder = ModelBuilder.build_encoder(\n","    arch='resnet50dilated',\n","    fc_dim=2048,\n","    # weights=weights_encoder\n","    weights='/Users/yoon/Documents/gatech/spring24/WallAI/semantic_segmentation_pytorch/ckpt/ade20k-resnet50dilated-ppm_deepsup/encoder_epoch_20.pth')\n","pspnet_decoder = ModelBuilder.build_decoder(\n","    arch='ppm_deepsup',\n","    fc_dim=2048,\n","    num_class=150,\n","    # weights=weights_decoder,\n","    weights='/Users/yoon/Documents/gatech/spring24/WallAI/semantic_segmentation_pytorch/ckpt/ade20k-resnet50dilated-ppm_deepsup/decoder_epoch_20.pth',\n","    use_softmax=True)\n","\n","crit = torch.nn.NLLLoss(ignore_index=-1)\n","segmentation_module = SegmentationModule(net_encoder, net_decoder, crit)#.to(DEVICE)\n","segmentation_module.eval()\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","segmentation_module.to(DEVICE)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32105,"status":"ok","timestamp":1713278856245,"user":{"displayName":"Garrett Devaney","userId":"10600170082792839935"},"user_tz":240},"id":"yLj6_bcu6bIt","outputId":"70a02b8f-2b3d-4a27-868b-3b62c813c159"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive/\n","/content/gdrive/My Drive/CV Project\n"]}],"source":["# from google.colab import drive\n","# drive.mount('/content/gdrive/', force_remount=True)\n","# %cd /content/gdrive/My Drive/CV Project"]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":842,"status":"ok","timestamp":1713278857083,"user":{"displayName":"Garrett Devaney","userId":"10600170082792839935"},"user_tz":240},"id":"h_57PWoUyz3h","outputId":"8bcbdbf6-4bce-481a-c474-de9725260d3b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Building encoder: resnet50-dilated\n","Loading weights for net_encoder\n","Loading weights for net_decoder\n"]}],"source":["#pspnet\n","os.chdir('/Users/yoon/Documents/gatech/spring24/WallAI/WallSegmentation')\n","from models.models import SegmentationModule, build_encoder, build_decoder\n","from src.eval import segment_image\n","\n","# Model weights (encoder and decoder)\n","weights_encoder = '/Users/yoon/Documents/gatech/spring24/WallSegmentation/model_weights/Transfer learning - entire decoder/transfer_encoder.pth'\n","weights_decoder = '/Users/yoon/Documents/gatech/spring24/WallSegmentation/model_weights/Transfer learning - entire decoder/transfer_decoder.pth'\n","\n","\n","net_encoder = build_encoder(weights_encoder)\n","net_decoder = build_decoder(weights_decoder)\n","\n","psp_segmentation_module = SegmentationModule(net_encoder, net_decoder)\n","psp_segmentation_module = psp_segmentation_module.to(DEVICE).eval()\n","os.chdir('/Users/yoon/Documents/gatech/spring24/WallAI/semantic_segmentation_pytorch')"]},{"cell_type":"code","execution_count":42,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1713278857083,"user":{"displayName":"Garrett Devaney","userId":"10600170082792839935"},"user_tz":240},"id":"SNmXL2ck5BLr"},"outputs":[],"source":["# Load and normalize one image as a singleton tensor batch\n","pil_to_tensor = torchvision.transforms.Compose([\n","    torchvision.transforms.ToTensor(),\n","    torchvision.transforms.Normalize(\n","        mean=[0.485, 0.456, 0.406], # These are RGB mean+std values\n","        std=[0.229, 0.224, 0.225])  # across a large photo dataset.\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":43,"metadata":{"executionInfo":{"elapsed":616,"status":"ok","timestamp":1713282820550,"user":{"displayName":"Garrett Devaney","userId":"10600170082792839935"},"user_tz":240},"id":"DCTktuhe9X1C"},"outputs":[],"source":["def readImage(img_name, psp):\n","    pil_image = PIL.Image.open(img_name).convert('RGB')\n","    img_original = np.array(pil_image)\n","    img_data = pil_to_tensor(pil_image)\n","\n","    \n","    singleton_batch = {'img_data': img_data[None].to(DEVICE)}\n","    \n","    output_size = img_data.shape[1:]\n","    with torch.no_grad():\n","        if psp:\n","             img = segment_image(segmentation_module, img_original)\n","            #  scores = psp_segmentation_module(singleton_batch, segSize=output_size)\n","        else:\n","            scores = segmentation_module(singleton_batch, segSize=output_size)\n","\n","    # Get the predicted scores for each pixel\n","    _, model_pred = torch.max(scores, dim=1)\n","    model_pred = model_pred.cpu()[0].numpy()\n","    model_pred[model_pred != 0] = -1\n","\n","    model_pred = colorEncode(model_pred, colors).astype(np.uint8)\n","    return img_original, model_pred\n","\n","def resizeAndPad(img):\n","    height, width = img.shape[:2]\n","    scaled_height, scaled_width = height+2, width+2\n","    interpolate = cv2.INTER_CUBIC\n","    aspect = width/height\n","    if aspect > 1:\n","        new_width = scaled_width\n","        new_height = np.round(new_width/aspect).astype(int)\n","        pad_vertical = (scaled_height-new_height)/2\n","        pad_top, pad_bottom = np.floor(pad_vertical).astype(int), np.ceil(pad_vertical).astype(int)\n","        pad_left, pad_right = 0, 0\n","    elif aspect < 1:\n","        new_height = scaled_height\n","        new_width = np.round(new_height*aspect).astype(int)\n","        pad_horizontal = (scaled_width-new_width)/2\n","        pad_left, pad_right = np.floor(pad_horizontal).astype(int), np.ceil(pad_horizontal).astype(int)\n","        pad_top, pad_bottom = 0, 0\n","    else:\n","        new_height, new_width = scaled_height, scaled_width\n","        pad_left, pad_right, pad_top, pad_bottom = 0, 0, 0, 0\n","    pad_color = [255]*3\n","    scaled_img = cv2.resize(img, (new_width, new_height), interpolation=interpolate)\n","    scaled_img = cv2.copyMakeBorder(scaled_img, pad_top, pad_bottom, pad_left, pad_right, borderType=cv2.BORDER_CONSTANT, value=pad_color)\n","    return scaled_img\n","\n","def getOutlineImg(img):\n","    # gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","    return cv2.Canny(img,60,180)\n","\n","def getColoredImage(img, new_color):\n","    hsv_img = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n","    h, s, v = cv2.split(hsv_img)\n","    hsv_color = cv2.cvtColor(np.uint8([[new_color]]), cv2.COLOR_RGB2HSV)\n","    h.fill(hsv_color[0][0][0])\n","    s.fill(hsv_color[0][0][1])\n","    new_hsv_img = cv2.merge([h, s, v])\n","    new_rgb_img = cv2.cvtColor(new_hsv_img, cv2.COLOR_HSV2RGB)\n","    return new_rgb_img\n","\n","def selectWall(outline_img, position):\n","    wall = outline_img.copy()\n","    scaled_mask = resizeAndPad(outline_img)\n","    for p in position:\n","        cv2.floodFill(wall, scaled_mask, p, 255)\n","    cv2.subtract(wall, outline_img, wall)\n","    return wall\n","\n","def getSamples(mask):\n","    gray = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n","    population = np.argwhere(gray == 120)\n","    s = np.log(len(population)).astype(int)\n","    i = np.random.choice(len(population), s, replace=False)\n","    final_inds = population[i]\n","    final_inds = [(y,x) for x,y in final_inds]\n","    return final_inds\n","\n","def mergeMasks(mask1, mask2):\n","    img = cv2.bitwise_or(mask1, mask2)\n","    return img\n","\n","def mergeImages(img, colored_img, wall):\n","    colored_img = cv2.bitwise_and(colored_img, colored_img, mask=wall)\n","    marked_img = cv2.bitwise_and(img, img, mask=cv2.bitwise_not(wall))\n","    final_img = cv2.bitwise_xor(colored_img, marked_img)\n","    return final_img\n","\n","def saveImage(img_name, img):\n","    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n","    img_name= img_name[8:]\n","    cv2.imwrite(\"./resnetcv/\" + img_name, img)\n","\n","def changeColor(img_name, new_color, psp=False):\n","    img_original, model_pred = readImage(img_name, psp)\n","    colored_img = getColoredImage(img_original, new_color)\n","    outline_img = getOutlineImg(img_original)\n","    pred_canny = getOutlineImg(model_pred)\n","    merge_mask = mergeMasks(pred_canny, outline_img)\n","    inds = getSamples(model_pred)\n","    selected_wall = selectWall(merge_mask, inds)\n","    final_img = mergeImages(img_original, colored_img, selected_wall)\n","    saveImage(img_name, final_img)\n","    return final_img, selected_wall\n","\n","def stats(pred_mask, true_mask):\n","    total_pixels = pred_mask.shape[0] * pred_mask.shape[1]\n","    print(f'total of pred: {total_pixels}')\n","    print(f'total of true: {true_mask.shape[0] * true_mask.shape[1]}')\n","    pred_mask = np.where(pred_mask == 255, 1, 0)\n","    print('weifoew: ', pred_mask)\n","    true_mask = np.where(true_mask == 255, 1, 0)\n","    accuracy = np.sum(pred_mask == true_mask) / total_pixels\n","    precision = np.sum(pred_mask & true_mask) / np.sum(pred_mask)\n","    recall = np.sum(pred_mask & true_mask) / np.sum(true_mask)\n","    return accuracy, precision, recall\n","\n","def IOU(pred_mask, true_mask):\n","    \"\"\"\n","        Function for calculating IOU of an image\n","    \"\"\"\n","\n","    pred_mask = np.where(pred_mask == 255, 1, 0)\n","    true_mask = np.where(true_mask == 255, 1, 0)\n","\n","    intersection = np.sum((pred_mask == 1) & (true_mask == 1))\n","    union = np.sum((pred_mask == 1) | (true_mask == 1))\n","    \n","    # ((pred_mask == 0) + (true_mask == 0)).sum() + 1e-15  # protection from division with 0\n","\n","    return intersection / (union + 1e-15)\n","\n","\n","def combine_masks(masks):\n","    combined_mask = masks[0]\n","    for mask in masks[1:]:\n","        combined_mask = cv2.bitwise_or(combined_mask, mask)\n","    return combined_mask\n","\n","from matplotlib import pyplot"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["iteration 1 iou: 0.2281711231869306\n","iteration 2 iou: 0.9403951880681509\n","iteration 3 iou: 0.7884208434171607\n","iteration 4 iou: 0.954853055959066\n","iteration 5 iou: 0.7159037340667643\n","iteration 6 iou: 0.7619780404257616\n","iteration 7 iou: 0.8281981563399526\n","iteration 8 iou: 0.2637907684876088\n","iteration 9 iou: 0.5934304354181629\n","iteration 10 iou: 0.8260631763935837\n","iteration 11 iou: 0.4603055749760326\n","iteration 12 iou: 0.47312454883669275\n","iteration 13 iou: 0.7353166154280898\n","iteration 14 iou: 0.6227172578430156\n","iteration 15 iou: 0.8909820862583053\n","iteration 16 iou: 0.7301503094606543\n","iteration 17 iou: 0.4291044776119403\n","iteration 18 iou: 0.5814810011671638\n","iteration 19 iou: 0.5247390597890447\n","iteration 20 iou: 0.930673014371045\n","iteration:  20\n","iteration 21 iou: 0.8296782469643453\n","iteration 22 iou: 0.9271442945100263\n","iteration 23 iou: 0.6448712467523925\n","iteration 24 iou: 0.6095305981440828\n","iteration 25 iou: 0.7453907346747777\n","iteration 26 iou: 0.7161987207976658\n","iteration 27 iou: 0.880158149070142\n","iteration 28 iou: 0.8508569299552906\n","iteration 29 iou: 0.8049008247700102\n","iteration 30 iou: 0.8818335503620953\n","iteration 31 iou: 0.80626051342061\n","iteration 32 iou: 0.8774172342640104\n","iteration 33 iou: 0.6341165503844101\n","iteration 34 iou: 0.3448241506426223\n","iteration 35 iou: 0.9611833518634891\n","iteration 36 iou: 0.9319118810644235\n","iteration 37 iou: 0.8258612295513245\n","iteration 38 iou: 0.7910424611791326\n","iteration 39 iou: 0.7096966911764706\n","iteration 40 iou: 0.7878604369497368\n","iteration:  40\n","iteration 41 iou: 0.6598758443437424\n","iteration 42 iou: 0.6779578872666534\n","iteration 43 iou: 0.48028935484152746\n","iteration 44 iou: 0.8677897508410941\n","iteration 45 iou: 0.9288367035541786\n","iteration 46 iou: 0.8511142721669037\n","iteration 47 iou: 0.7845545948906537\n","iteration 48 iou: 0.8131058835671475\n","iteration 49 iou: 0.47327352946245455\n","iteration 50 iou: 0.8919954926856752\n","iteration 51 iou: 0.8396186144414269\n","iteration 52 iou: 0.6971991114966549\n","iteration 53 iou: 0.9171322843395218\n","iteration 54 iou: 0.9532334025421583\n","iteration 55 iou: 0.8691751440127466\n","iteration 56 iou: 0.8268411082169074\n","iteration 57 iou: 0.8465983566650938\n","iteration 58 iou: 0.6323352411203005\n","iteration 59 iou: 0.8635219597371334\n","iteration 60 iou: 0.6258501553638556\n","iteration:  60\n","iteration 61 iou: 0.8877042704478356\n","iteration 62 iou: 0.5891900226697855\n","iteration 63 iou: 0.9406012744881858\n","iteration 64 iou: 0.8313260213382744\n","iteration 65 iou: 0.5678940355346177\n","iteration 66 iou: 0.7483853844112183\n","iteration 67 iou: 0.8228257328153984\n","iteration 68 iou: 0.1870230375013957\n","iteration 69 iou: 0.8158043087080641\n","iteration 70 iou: 0.9201056163428454\n","iteration 71 iou: 0.8962025316455696\n","iteration 72 iou: 0.7922043325917933\n","iteration 73 iou: 0.9573084262728102\n","iteration 74 iou: 0.9051981864000876\n","iteration 75 iou: 0.8820433142963254\n","iteration 76 iou: 0.8244631760290408\n","iteration 77 iou: 0.6477527187014518\n","iteration 78 iou: 0.8410809470683029\n","iteration 79 iou: 0.7555407679962423\n","iteration 80 iou: 0.8062313093812615\n","iteration:  80\n","iteration 81 iou: 0.8233983248848871\n","iteration 82 iou: 0.3545314126233866\n","iteration 83 iou: 0.3714698369500346\n","iteration 84 iou: 0.6637299101765496\n","iteration 85 iou: 0.09610864455471402\n","iteration 86 iou: 0.7950344611528822\n","iteration 87 iou: 0.9250030143025814\n","iteration 88 iou: 0.39040304054393676\n","iteration 89 iou: 0.7176379861187617\n","iteration 90 iou: 0.7676993131323467\n","iteration 91 iou: 0.6921233508018856\n","iteration 92 iou: 0.7825568058526303\n","iteration 93 iou: 0.8297923599295318\n","iteration 94 iou: 0.8456528253087101\n","iteration 95 iou: 0.6806267203048909\n","iteration 96 iou: 0.46495613599573254\n","iteration 97 iou: 0.8935946038539087\n","iteration 98 iou: 0.8096894640413739\n","iteration 99 iou: 0.8181722107906826\n","iteration 100 iou: 0.9287889005091843\n","iteration:  100\n","Accuracy: 0.9024727034256457\n","Precision:  0.873617613217152\n","Recall:  0.830076256588707\n","F1:  0.8512905410076866\n","IOU:  0.7361068969202711\n"]}],"source":["# segmentation\n","new_color = [111, 209, 201]\n","total_accuracy = 0\n","total_precision = 0\n","total_recall = 0\n","total_iou = 0\n","for i in range(1,101):\n","    # m = 'bedroom/' + str(i) + 'wall*'\n","    m = '/Users/yoon/Documents/gatech/spring24/WallAI/Data/bedroom/' + str(i) + 'wall*'\n","    masks = glob.glob(m)\n","    # print(masks)\n","    # j = 'bedroom/' + str(i) + '_img*'\n","    j = '/Users/yoon/Documents/gatech/spring24/WallAI/Data/bedroom/' + str(i) + '_img*'\n","    img = glob.glob(j)\n","    masks = [cv2.imread(mask) for mask in masks]\n","    masks = [cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY) for mask in masks]\n","    final_mask = masks[0]\n","    if len(masks) > 1:\n","        for mask in masks[1:]:\n","            final_mask = cv2.bitwise_or(final_mask, mask)\n","\n","    final_img, wall = changeColor(img[0],new_color, psp=False)\n","    \n","    acc, prec, rec = stats(wall, final_mask)\n","    total_accuracy += acc\n","    total_precision += prec\n","    total_recall += rec\n","\n","    iou = IOU(wall, final_mask)\n","    print(f'iteration {i} iou: {iou}')\n","    # break\n","    total_iou += iou\n","\n","    if i % 20 == 0:\n","        print('iteration: ', i)\n","\n","total_accuracy /= 100\n","total_precision /= 100\n","total_recall /= 100\n","total_iou /= 100\n","print('Accuracy:', total_accuracy)\n","print('Precision: ', total_precision)\n","print('Recall: ', total_recall)\n","print('F1: ', 2 * (total_precision * total_recall) / (total_precision + total_recall))\n","print('IOU: ', total_iou)"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[],"source":["from PIL import Image\n","IMAGENET_MEAN = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n","IMAGENET_STD = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n","\n","\n","\n","def visualize_wall(img, pred, class_to_display=0):\n","    \"\"\"\n","        Function for visualizing wall prediction \n","        (original image, segmentation mask and original image with the segmented wall)\n","    \"\"\"\n","    img_green = img.copy()\n","\n","    wall_pixels = pred == class_to_display\n","    \n","    # Retain original pixels where it's not a wall\n","    img_green[~wall_pixels] = img[~wall_pixels]\n","    \n","    # Create a black background\n","    black_background = np.zeros_like(img)\n","    \n","    # Color the wall pixels with green\n","    black_background[wall_pixels] = [255, 0, 0]\n","    black_background[~wall_pixels] = [0,0,0]\n","    # black_background[wall_pixels] = [111, 209, 201]\n","\n","    return np.array(PIL.Image.fromarray(black_background))\n","    \n","def segment_image(segmentation_module, img, disp_image=True):\n","    \"\"\"\n","        Function for segmenting wall in the input image. The input can be path to image, or a loaded image\n","    \"\"\"\n","    pil_to_tensor = torchvision.transforms.Compose([\n","        torchvision.transforms.ToTensor(),\n","        torchvision.transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n","    ])\n","\n","    if isinstance(img, str):\n","        img = Image.open(img)\n","    \n","    img_original = np.array(img)\n","    img_data = pil_to_tensor(img)\n","    singleton_batch = {'img_data': img_data[None].to(DEVICE)}\n","    seg_size = img_original.shape[:2]\n","\n","    with torch.no_grad():\n","        scores = segmentation_module(singleton_batch, seg_size=seg_size)\n","\n","    _, pred = torch.max(scores, dim=1)\n","    pred = pred.cpu()[0].numpy()\n","\n","    if disp_image:\n","        return visualize_wall(img_original, pred)\n","    else:\n","        print(\"WEOIFJEWOIFJ\")\n","        \n","    return pred"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ffweafwaefwef\n","torch.Size([1, 2048, 207, 275])\n","ffweafwaefwef\n","torch.Size([1, 2048, 106, 150])\n","ffweafwaefwef\n","torch.Size([1, 2048, 96, 128])\n","ffweafwaefwef\n","torch.Size([1, 2048, 150, 200])\n","ffweafwaefwef\n","torch.Size([1, 2048, 192, 256])\n","ffweafwaefwef\n","torch.Size([1, 2048, 108, 144])\n","ffweafwaefwef\n","torch.Size([1, 2048, 150, 200])\n","ffweafwaefwef\n","torch.Size([1, 2048, 78, 117])\n","ffweafwaefwef\n","torch.Size([1, 2048, 192, 256])\n","ffweafwaefwef\n","torch.Size([1, 2048, 84, 128])\n","iteration:  10\n","ffweafwaefwef\n","torch.Size([1, 2048, 207, 275])\n","ffweafwaefwef\n","torch.Size([1, 2048, 62, 94])\n","ffweafwaefwef\n","torch.Size([1, 2048, 75, 113])\n","ffweafwaefwef\n","torch.Size([1, 2048, 192, 256])\n","ffweafwaefwef\n","torch.Size([1, 2048, 120, 160])\n","ffweafwaefwef\n","torch.Size([1, 2048, 67, 100])\n","ffweafwaefwef\n","torch.Size([1, 2048, 32, 32])\n","ffweafwaefwef\n","torch.Size([1, 2048, 38, 57])\n","ffweafwaefwef\n","torch.Size([1, 2048, 150, 100])\n","ffweafwaefwef\n","torch.Size([1, 2048, 109, 164])\n","iteration:  20\n","ffweafwaefwef\n","torch.Size([1, 2048, 60, 80])\n","ffweafwaefwef\n","torch.Size([1, 2048, 186, 275])\n","ffweafwaefwef\n","torch.Size([1, 2048, 75, 113])\n","ffweafwaefwef\n","torch.Size([1, 2048, 184, 275])\n","ffweafwaefwef\n","torch.Size([1, 2048, 40, 30])\n","ffweafwaefwef\n","torch.Size([1, 2048, 67, 100])\n","ffweafwaefwef\n","torch.Size([1, 2048, 66, 100])\n","ffweafwaefwef\n","torch.Size([1, 2048, 29, 38])\n","ffweafwaefwef\n","torch.Size([1, 2048, 207, 275])\n","ffweafwaefwef\n","torch.Size([1, 2048, 96, 128])\n","iteration:  30\n","ffweafwaefwef\n","torch.Size([1, 2048, 32, 32])\n","ffweafwaefwef\n","torch.Size([1, 2048, 77, 169])\n","ffweafwaefwef\n","torch.Size([1, 2048, 207, 275])\n","ffweafwaefwef\n","torch.Size([1, 2048, 32, 43])\n","ffweafwaefwef\n","torch.Size([1, 2048, 83, 150])\n","ffweafwaefwef\n","torch.Size([1, 2048, 120, 160])\n","ffweafwaefwef\n","torch.Size([1, 2048, 102, 150])\n","ffweafwaefwef\n","torch.Size([1, 2048, 87, 125])\n","ffweafwaefwef\n","torch.Size([1, 2048, 32, 32])\n","ffweafwaefwef\n","torch.Size([1, 2048, 54, 80])\n","iteration:  40\n","ffweafwaefwef\n","torch.Size([1, 2048, 192, 256])\n","ffweafwaefwef\n","torch.Size([1, 2048, 69, 85])\n","ffweafwaefwef\n","torch.Size([1, 2048, 207, 275])\n","ffweafwaefwef\n","torch.Size([1, 2048, 44, 64])\n","ffweafwaefwef\n","torch.Size([1, 2048, 189, 275])\n","ffweafwaefwef\n","torch.Size([1, 2048, 54, 80])\n","ffweafwaefwef\n","torch.Size([1, 2048, 184, 275])\n","ffweafwaefwef\n","torch.Size([1, 2048, 54, 80])\n","ffweafwaefwef\n","torch.Size([1, 2048, 192, 256])\n","ffweafwaefwef\n","torch.Size([1, 2048, 60, 80])\n","iteration:  50\n","ffweafwaefwef\n","torch.Size([1, 2048, 128, 96])\n","ffweafwaefwef\n","torch.Size([1, 2048, 75, 109])\n","ffweafwaefwef\n","torch.Size([1, 2048, 60, 80])\n","ffweafwaefwef\n","torch.Size([1, 2048, 60, 80])\n","ffweafwaefwef\n","torch.Size([1, 2048, 96, 128])\n","ffweafwaefwef\n","torch.Size([1, 2048, 32, 32])\n","ffweafwaefwef\n","torch.Size([1, 2048, 154, 242])\n","ffweafwaefwef\n","torch.Size([1, 2048, 192, 256])\n","ffweafwaefwef\n","torch.Size([1, 2048, 151, 100])\n","ffweafwaefwef\n","torch.Size([1, 2048, 86, 128])\n","iteration:  60\n","ffweafwaefwef\n","torch.Size([1, 2048, 67, 100])\n","ffweafwaefwef\n","torch.Size([1, 2048, 192, 256])\n","ffweafwaefwef\n","torch.Size([1, 2048, 207, 275])\n","ffweafwaefwef\n","torch.Size([1, 2048, 128, 96])\n","ffweafwaefwef\n","torch.Size([1, 2048, 108, 163])\n","ffweafwaefwef\n","torch.Size([1, 2048, 32, 32])\n","ffweafwaefwef\n","torch.Size([1, 2048, 207, 275])\n","ffweafwaefwef\n","torch.Size([1, 2048, 207, 275])\n","ffweafwaefwef\n","torch.Size([1, 2048, 77, 113])\n","ffweafwaefwef\n","torch.Size([1, 2048, 120, 160])\n","iteration:  70\n","ffweafwaefwef\n","torch.Size([1, 2048, 67, 100])\n","ffweafwaefwef\n","torch.Size([1, 2048, 207, 275])\n","ffweafwaefwef\n","torch.Size([1, 2048, 192, 256])\n","ffweafwaefwef\n","torch.Size([1, 2048, 86, 125])\n","ffweafwaefwef\n","torch.Size([1, 2048, 181, 241])\n","ffweafwaefwef\n","torch.Size([1, 2048, 62, 83])\n","ffweafwaefwef\n","torch.Size([1, 2048, 86, 128])\n","ffweafwaefwef\n","torch.Size([1, 2048, 50, 50])\n","ffweafwaefwef\n","torch.Size([1, 2048, 100, 150])\n","ffweafwaefwef\n","torch.Size([1, 2048, 96, 128])\n","iteration:  80\n","ffweafwaefwef\n","torch.Size([1, 2048, 207, 275])\n","ffweafwaefwef\n","torch.Size([1, 2048, 184, 275])\n","ffweafwaefwef\n","torch.Size([1, 2048, 120, 160])\n","ffweafwaefwef\n","torch.Size([1, 2048, 29, 38])\n","ffweafwaefwef\n","torch.Size([1, 2048, 24, 32])\n","ffweafwaefwef\n","torch.Size([1, 2048, 50, 75])\n","ffweafwaefwef\n","torch.Size([1, 2048, 192, 256])\n","ffweafwaefwef\n","torch.Size([1, 2048, 207, 275])\n","ffweafwaefwef\n","torch.Size([1, 2048, 42, 63])\n","ffweafwaefwef\n","torch.Size([1, 2048, 74, 100])\n","iteration:  90\n","ffweafwaefwef\n","torch.Size([1, 2048, 47, 62])\n","ffweafwaefwef\n","torch.Size([1, 2048, 207, 275])\n","ffweafwaefwef\n","torch.Size([1, 2048, 58, 88])\n","ffweafwaefwef\n","torch.Size([1, 2048, 42, 63])\n","ffweafwaefwef\n","torch.Size([1, 2048, 28, 41])\n","ffweafwaefwef\n","torch.Size([1, 2048, 200, 200])\n","ffweafwaefwef\n","torch.Size([1, 2048, 207, 275])\n","ffweafwaefwef\n","torch.Size([1, 2048, 43, 64])\n","ffweafwaefwef\n","torch.Size([1, 2048, 120, 160])\n","ffweafwaefwef\n","torch.Size([1, 2048, 129, 161])\n","iteration:  100\n","Accuracy: 0.8599921987987367\n","Precision:  0.768793790779977\n","Recall:  0.8581949878960546\n","F1:  0.8110381418977933\n","IOU:  0.6713519473958601\n"]}],"source":["#pspnet\n","\n","def stats1(pred_mask, true_mask):\n","    total_pixels = pred_mask.shape[0] * pred_mask.shape[1]\n","    # print(f'total of pred: {total_pixels}')\n","    # print(f'total of true: {true_mask.shape[0] * true_mask.shape[1]}')\n","    pred_mask = np.where(pred_mask == 255, 1, 0)\n","    # print('weifoew: ', pred_mask)\n","    true_mask = np.where(true_mask == 255, 1, 0)\n","    accuracy = np.sum(pred_mask == true_mask) / total_pixels\n","    precision = np.sum(pred_mask & true_mask) / np.sum(pred_mask)\n","    recall = np.sum(pred_mask & true_mask) / np.sum(true_mask)\n","    return accuracy, precision, recall\n","\n","new_color = [111, 209, 201]\n","total_accuracy = 0\n","total_precision = 0\n","total_recall = 0\n","total_iou = 0\n","for i in range(1,101):\n","# for i in range(1,11):\n","    # m = 'bedroom/' + str(i) + 'wall*'\n","    m = '/Users/yoon/Documents/gatech/spring24/WallAI/Data/bedroom/' + str(i) + 'wall*'\n","    masks = glob.glob(m)\n","    # print(masks)\n","    # j = 'bedroom/' + str(i) + '_img*'\n","    j = '/Users/yoon/Documents/gatech/spring24/WallAI/Data/bedroom/' + str(i) + '_img*'\n","    img = glob.glob(j)\n","    masks = [cv2.imread(mask) for mask in masks]\n","    masks = [cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY) for mask in masks]\n","    final_mask = masks[0]\n","    if len(masks) > 1:\n","        for mask in masks[1:]:\n","            final_mask = cv2.bitwise_or(final_mask, mask)\n","\n","    \n","    # final_img, wall = changeColor(img[0], new_color)\n","\n","    wall = segment_image(psp_segmentation_module, img[0])\n","    wall = wall[:,:,0]\n","    # wall = cv2.imread(wall)\n","    # wall = cv2.cvtColor(wall, cv2.COLOR_BGR2GRAY)\n","\n","    # pyplot.imshow(final_mask)\n","    # break\n","\n","    \n","    acc, prec, rec = stats1(wall, final_mask)\n","    total_accuracy += acc\n","    total_precision += prec\n","    total_recall += rec\n","\n","    iou = IOU(wall, final_mask)\n","    total_iou += iou\n","    # break\n","    if i % 10 == 0:\n","        print('iteration: ', i)\n","\n","total_accuracy /= 100\n","total_precision /= 100\n","total_recall /= 100\n","total_iou /= 100\n","print('Accuracy:', total_accuracy)\n","print('Precision: ', total_precision)\n","print('Recall: ', total_recall)\n","print('F1: ', 2 * (total_precision * total_recall) / (total_precision + total_recall))\n","print('IOU: ', total_iou)"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":74086,"status":"ok","timestamp":1713283185871,"user":{"displayName":"Garrett Devaney","userId":"10600170082792839935"},"user_tz":240},"id":"mFue6Ikmc4hr","outputId":"2e53b9df-1424-44b4-e456-2a15ee4cb652"},"outputs":[{"name":"stdout","output_type":"stream","text":["iteration:  20\n","iteration:  40\n","iteration:  60\n","iteration:  80\n","iteration:  100\n","Accuracy: 0.8930769958447106\n","Precision:  0.8607124761024225\n","Recall:  0.8277385887280082\n","F1:  0.8439035576564474\n"]}],"source":["#original\n","\n","new_color = [111, 209, 201]\n","total_accuracy = 0\n","total_precision = 0\n","total_recall = 0\n","for i in range(1,101):\n","    m = 'bedroom/' + str(i) + 'wall*'\n","    masks = glob.glob(m)\n","    j = 'bedroom/' + str(i) + '_img*'\n","    img = glob.glob(j)\n","    masks = [cv2.imread(mask) for mask in masks]\n","    masks = [cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY) for mask in masks]\n","    final_mask = masks[0]\n","    if len(masks) > 1:\n","        for mask in masks[1:]:\n","            final_mask = cv2.bitwise_or(final_mask, mask)\n","\n","    final_img, wall = changeColor(img[0],new_color)\n","    acc, prec, rec = stats(wall, final_mask)\n","    total_accuracy += acc\n","    total_precision += prec\n","    total_recall += rec\n","    if i % 20 == 0:\n","        print('iteration: ', i)\n","\n","total_accuracy /= 100\n","total_precision /= 100\n","total_recall /= 100\n","print('Accuracy:', total_accuracy)\n","print('Precision: ', total_precision)\n","print('Recall: ', total_recall)\n","print('F1: ', 2 * (total_precision * total_recall) / (total_precision + total_recall))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rDON8gjSoZfC"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMmBPbq8bmwjsbe3XS4Bx/e","gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":0}
